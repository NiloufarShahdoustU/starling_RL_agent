{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a6988d",
   "metadata": {},
   "source": [
    "# TD learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7001df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cc3e4",
   "metadata": {},
   "source": [
    "# reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d47cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 participants.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "arrowRT",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "distribution",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "interTrialInterval",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "outcome",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "myCard",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "yourCard",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "spaceRT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "totalReward",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "trialIndex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trialType",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "choice",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "block",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timeoutRepeat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "risk",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "98081c5a-ffe1-4766-af0e-e798669564e8",
       "rows": [
        [
         "0",
         "570",
         "uniform",
         "831",
         "lose",
         "5",
         "2",
         "2209",
         "9.5",
         "0",
         "response",
         "arrowdown",
         "1",
         "0",
         "0.5"
        ],
        [
         "1",
         "1162",
         "uniform",
         "901",
         "lose",
         "4",
         "3",
         "5755",
         "9",
         "1",
         "response",
         "arrowdown",
         "1",
         "0",
         "0.375"
        ],
        [
         "2",
         "355",
         "uniform",
         "939",
         "win",
         "4",
         "6",
         "1209",
         "9.5",
         "2",
         "response",
         "arrowdown",
         "1",
         "0",
         "0.375"
        ],
        [
         "3",
         "1163",
         "uniform",
         "828",
         "win",
         "7",
         "5",
         "1997",
         "10",
         "3",
         "response",
         "arrowup",
         "1",
         "0",
         "0.25"
        ],
        [
         "4",
         "299",
         "uniform",
         "776",
         "win",
         "3",
         "9",
         "1324",
         "10.5",
         "4",
         "response",
         "arrowdown",
         "1",
         "0",
         "0.25"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrowRT</th>\n",
       "      <th>distribution</th>\n",
       "      <th>interTrialInterval</th>\n",
       "      <th>outcome</th>\n",
       "      <th>myCard</th>\n",
       "      <th>yourCard</th>\n",
       "      <th>spaceRT</th>\n",
       "      <th>totalReward</th>\n",
       "      <th>trialIndex</th>\n",
       "      <th>trialType</th>\n",
       "      <th>choice</th>\n",
       "      <th>block</th>\n",
       "      <th>timeoutRepeat</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570</td>\n",
       "      <td>uniform</td>\n",
       "      <td>831</td>\n",
       "      <td>lose</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2209</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "      <td>response</td>\n",
       "      <td>arrowdown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1162</td>\n",
       "      <td>uniform</td>\n",
       "      <td>901</td>\n",
       "      <td>lose</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5755</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>response</td>\n",
       "      <td>arrowdown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>355</td>\n",
       "      <td>uniform</td>\n",
       "      <td>939</td>\n",
       "      <td>win</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1209</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2</td>\n",
       "      <td>response</td>\n",
       "      <td>arrowdown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1163</td>\n",
       "      <td>uniform</td>\n",
       "      <td>828</td>\n",
       "      <td>win</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>response</td>\n",
       "      <td>arrowup</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299</td>\n",
       "      <td>uniform</td>\n",
       "      <td>776</td>\n",
       "      <td>win</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1324</td>\n",
       "      <td>10.5</td>\n",
       "      <td>4</td>\n",
       "      <td>response</td>\n",
       "      <td>arrowdown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  arrowRT distribution  interTrialInterval outcome  myCard  yourCard  spaceRT  \\\n",
       "0     570      uniform                 831    lose       5         2     2209   \n",
       "1    1162      uniform                 901    lose       4         3     5755   \n",
       "2     355      uniform                 939     win       4         6     1209   \n",
       "3    1163      uniform                 828     win       7         5     1997   \n",
       "4     299      uniform                 776     win       3         9     1324   \n",
       "\n",
       "  totalReward  trialIndex trialType     choice  block  timeoutRepeat   risk  \n",
       "0         9.5           0  response  arrowdown      1              0  0.500  \n",
       "1           9           1  response  arrowdown      1              0  0.375  \n",
       "2         9.5           2  response  arrowdown      1              0  0.375  \n",
       "3          10           3  response    arrowup      1              0  0.250  \n",
       "4        10.5           4  response  arrowdown      1              0  0.250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "folder_path = 'data_risk_added'\n",
    "dataframes = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_excel(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "\n",
    "n_participant = len(dataframes)\n",
    "print(f\"There are {n_participant} participants.\")\n",
    "\n",
    "\n",
    "dataframes[0].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2b829",
   "metadata": {},
   "source": [
    "# reward model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_td_model(Reward, yBinary, alpha_values=None):\n",
    "    \"\"\"\n",
    "    a simple TD-learning model for each alpha in alpha_values,\n",
    "    then does logistic regression (outcome ~ final value estimate).\n",
    "    \n",
    "    returns:\n",
    "        alpha_values (np.ndarray)\n",
    "        inverseTemps (np.ndarray)\n",
    "        bestAlpha (float)\n",
    "        bestValEstimate (np.ndarray)\n",
    "        bestModelParams (statsmodels parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    if alpha_values is None:\n",
    "        alpha_values = np.arange(0.01, 1.01, 0.01)  # 0.01 to 1.0 in steps of 0.01\n",
    "\n",
    "    nTrials = len(Reward)\n",
    "    inverseTemps = np.zeros(len(alpha_values))\n",
    "\n",
    "    # 1) a TD update, then fit logistic regression for each alpha\n",
    "    for a_idx, alpha in enumerate(alpha_values):\n",
    "        \n",
    "        V = Reward[0]\n",
    "        val_estimates = np.zeros(nTrials)\n",
    "        val_estimates[0] = V\n",
    "        \n",
    "        for t in range(1, nTrials):\n",
    "            PE = Reward[t-1] - V\n",
    "            V = V + alpha * PE\n",
    "            val_estimates[t] = V\n",
    "        \n",
    "        # fit logistic regression: outcome ~ val_estimates\n",
    "        \n",
    "        X = sm.add_constant(val_estimates)\n",
    "        model = sm.Logit(yBinary, X)\n",
    "        try:\n",
    "            result = model.fit(disp=0)\n",
    "            slope = result.params[1]  # the second parameter is slope whkch I'm using for inverse temperature\n",
    "            inverseTemps[a_idx] = abs(slope)\n",
    "        except:\n",
    "            inverseTemps[a_idx] = 0  # in case fit fails for some alpha and gives fktard resutls\n",
    "\n",
    "    # 2) pick best alpha\n",
    "    best_idx = np.argmax(inverseTemps)\n",
    "    bestAlpha = alpha_values[best_idx]\n",
    "\n",
    "    # 3) recompute final value estimates for bestAlpha\n",
    "    best_val_est = np.zeros(nTrials)\n",
    "    best_val_est_PE = np.zeros(nTrials)\n",
    "    \n",
    "    V = Reward[0]\n",
    "    for t in range(1, nTrials):\n",
    "        PE = Reward[t-1] - V\n",
    "        V = V + bestAlpha * PE\n",
    "        best_val_est[t] = V\n",
    "        best_val_est_PE[t]= PE\n",
    "\n",
    "    # 4) fit logistic regression for bestAlpha\n",
    "    X_best = sm.add_constant(best_val_est)\n",
    "    best_model = sm.Logit(yBinary, X_best)\n",
    "    try:\n",
    "        best_result = best_model.fit(disp=0)\n",
    "        best_params = best_result.params\n",
    "    except:\n",
    "        best_params = [np.nan, np.nan] # in case of poopy results :|\n",
    "\n",
    "    return alpha_values, inverseTemps, bestAlpha, best_val_est, best_val_est_PE, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f06826",
   "metadata": {},
   "source": [
    "# run reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_dir = \"6_RL_agent_TDlearn_output/reward\"\n",
    "best_alpha_accros_participants = []\n",
    "\n",
    "for idx, df in enumerate(dataframes):\n",
    "\n",
    "    df = df[df['outcome'].str.lower() != 'na']\n",
    "    \n",
    "    reward_win = 0.5\n",
    "    reward_lose = -0.5\n",
    "\n",
    "    outcomes = df['outcome'].astype(str).values\n",
    "    nTrials = len(df)\n",
    "    reward = np.zeros(nTrials)\n",
    "    yBinary = np.zeros(nTrials, dtype=int)\n",
    "\n",
    "    for i, outcome in enumerate(outcomes):\n",
    "        if outcome.lower() == 'win':\n",
    "            reward[i] = reward_win\n",
    "            yBinary[i] = 1\n",
    "        elif outcome.lower() == 'lose':\n",
    "            reward[i] = reward_lose\n",
    "            yBinary[i] = 0\n",
    "\n",
    "\n",
    "    alpha_values, inverseTemps, bestAlpha, best_val_est, best_val_est_PE, best_params = fit_td_model(reward, yBinary)\n",
    "    \n",
    "    best_alpha_accros_participants.append(bestAlpha)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(14, 4), gridspec_kw={'width_ratios': [1, 1, 1, 0.5]})\n",
    "\n",
    "    axs[0].plot(alpha_values, inverseTemps, '-o', ms=4)\n",
    "    axs[0].set_xlabel('alpha')\n",
    "    axs[0].set_ylabel('inverse temperature')\n",
    "    axs[0].axvline(bestAlpha, color='r', linestyle='--', label=f'best alpha = {bestAlpha:.2f}')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(best_val_est, label='estimated value')\n",
    "    axs[1].plot(reward, drawstyle='steps-mid', alpha=0.5, label='reward')  \n",
    "    axs[1].set_xlabel('trial')\n",
    "    axs[1].set_ylabel('value / reward')\n",
    "    axs[1].set_title('best value estimate vs. actual reward')\n",
    "    axs[1].set_ylim(-0.6, 0.6)\n",
    "    axs[1].legend()\n",
    "    \n",
    "    axs[2].plot(best_val_est_PE)\n",
    "    axs[2].set_xlabel('trial')\n",
    "    axs[2].set_ylabel('PE')\n",
    "    axs[2].set_ylim(-1, 1)\n",
    "\n",
    "    axs[3].hist(best_val_est_PE, bins=20, orientation='horizontal', alpha=0.7)\n",
    "    axs[3].set_xlabel('count')\n",
    "    axs[3].set_ylabel('PE')\n",
    "    axs[3].set_ylim(-1, 1)\n",
    "    axs[3].set_title('PE histogram')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"plot_{idx}.pdf\")\n",
    "    plt.savefig(filename, format='pdf')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9e7ce",
   "metadata": {},
   "source": [
    "# Risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_td_model_risk(Risk, yBinary, alpha_values=None):\n",
    "    \"\"\"\n",
    "    A simple TD-learning model for each alpha in alpha_values,\n",
    "    then does logistic regression (outcome ~ final value estimate).\n",
    "    \n",
    "    Returns:\n",
    "        alpha_values (np.ndarray)\n",
    "        inverseTemps (np.ndarray)\n",
    "        bestAlpha (float)\n",
    "        bestValEstimate (np.ndarray)\n",
    "        bestModelParams (statsmodels parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    if alpha_values is None:\n",
    "        alpha_values = np.arange(0.01, 1.01, 0.01)  # 0.01 to 1.0 in steps of 0.01\n",
    "\n",
    "    nTrials = len(Risk)\n",
    "    inverseTemps = np.zeros(len(alpha_values))\n",
    "\n",
    "    # 1) a TD update, then fit logistic regression for each alpha\n",
    "    for a_idx, alpha in enumerate(alpha_values):\n",
    "        \n",
    "        V = Risk[0]\n",
    "        val_estimates = np.zeros(nTrials)\n",
    "        val_estimates[0] = V\n",
    "        \n",
    "        for t in range(1, nTrials):\n",
    "            PE = Risk[t-1] - V\n",
    "            V = V + alpha * PE\n",
    "            val_estimates[t] = V\n",
    "        \n",
    "        # fit logistic regression: outcome ~ val_estimates\n",
    "        \n",
    "        X = sm.add_constant(val_estimates)\n",
    "        model = sm.Logit(yBinary, X)\n",
    "        try:\n",
    "            result = model.fit(disp=0)\n",
    "            slope = result.params[1]  # the second parameter is slope used for inverse temperature\n",
    "            inverseTemps[a_idx] = abs(slope)\n",
    "        except:\n",
    "            inverseTemps[a_idx] = 0  # in case fit fails for some alpha\n",
    "\n",
    "    # 2) pick best alpha\n",
    "    best_idx = np.argmax(inverseTemps)\n",
    "    bestAlpha = alpha_values[best_idx]\n",
    "\n",
    "    # 3) recompute final value estimates for bestAlpha\n",
    "    best_val_est = np.zeros(nTrials)\n",
    "    best_val_est_PE = np.zeros(nTrials)\n",
    "    \n",
    "    V = Risk[0]\n",
    "    for t in range(1, nTrials):\n",
    "        PE = Risk[t-1] - V\n",
    "        V = V + bestAlpha * PE\n",
    "        best_val_est[t] = V\n",
    "        best_val_est_PE[t] = PE\n",
    "\n",
    "    # 4) fit logistic regression for bestAlpha\n",
    "    X_best = sm.add_constant(best_val_est)\n",
    "    best_model = sm.Logit(yBinary, X_best)\n",
    "    try:\n",
    "        best_result = best_model.fit(disp=0)\n",
    "        best_params = best_result.params\n",
    "    except:\n",
    "        best_params = [np.nan, np.nan] \n",
    "\n",
    "    return alpha_values, inverseTemps, bestAlpha, best_val_est, best_val_est_PE, best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca209632",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"6_RL_agent_TDlearn_output/risk\"\n",
    "best_alpha_accros_participants = []\n",
    "\n",
    "for idx, df in enumerate(dataframes):\n",
    "    df = df[df['outcome'].str.lower() != 'na']\n",
    "    \n",
    "    outcomes = df['outcome'].astype(str).values\n",
    "    nTrials = len(df)\n",
    "    risk = df['risk'].values  # Directly using the risk column\n",
    "    yBinary = np.zeros(nTrials, dtype=int)\n",
    "\n",
    "    for i, outcome in enumerate(outcomes):\n",
    "        if outcome.lower() == 'win':\n",
    "            yBinary[i] = 1\n",
    "        elif outcome.lower() == 'lose':\n",
    "            yBinary[i] = 0\n",
    "            \n",
    "\n",
    "    alpha_values, inverseTemps, bestAlpha, best_val_est, best_val_est_PE, best_params = fit_td_model(risk, yBinary)\n",
    "    \n",
    "    best_alpha_accros_participants.append(bestAlpha)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(14, 4), gridspec_kw={'width_ratios': [1, 1, 1, 0.5]})\n",
    "\n",
    "    axs[0].plot(alpha_values, inverseTemps, '-o', ms=4)\n",
    "    axs[0].set_xlabel('alpha')\n",
    "    axs[0].set_ylabel('inverse temperature')\n",
    "    axs[0].axvline(bestAlpha, color='r', linestyle='--', label=f'best alpha = {bestAlpha:.2f}')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(best_val_est, label='estimated value')\n",
    "    axs[1].plot(risk, drawstyle='steps-mid', alpha=0.5, label='risk')  \n",
    "    axs[1].set_xlabel('trial')\n",
    "    axs[1].set_ylabel('value / risk')\n",
    "    axs[1].set_title('best value estimate vs. actual risk')\n",
    "    axs[1].set_ylim(0, 0.5)\n",
    "    axs[1].legend()\n",
    "    \n",
    "    axs[2].plot(best_val_est_PE)\n",
    "    axs[2].set_xlabel('trial')\n",
    "    axs[2].set_ylabel('PE')\n",
    "    # axs[2].set_ylim(-1, 1)\n",
    "\n",
    "    axs[3].hist(best_val_est_PE, bins=20, orientation='horizontal', alpha=0.7)\n",
    "    axs[3].set_xlabel('count')\n",
    "    axs[3].set_ylabel('PE')\n",
    "    # axs[3].set_ylim(-1, 1)\n",
    "    axs[3].set_title('PE histogram')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"plot_{idx}.pdf\")\n",
    "    plt.savefig(filename, format='pdf')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7eef48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
