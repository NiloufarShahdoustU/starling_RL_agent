{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a6988d",
   "metadata": {},
   "source": [
    "# TD learning; the agent that can see\n",
    "\n",
    "# Remember to check the number of samples for alpha and beta\n",
    "\n",
    "now I'm gonna add numbers to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7001df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(42)\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "import matplotlib.ticker as mticker\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data_risk_added'\n",
    "dataframes = [pd.read_excel(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "n_participant = len(dataframes)\n",
    "print(f\"there are {n_participant} participants.\")\n",
    "\n",
    "\n",
    "output_dir = \"13_RL_agent_TDlearn_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dataframes[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4cf94",
   "metadata": {},
   "source": [
    "# policy initilization for the model\n",
    "now I need to find the prior policy amounts. for that I am going to put the percentage of downarrow and up arrow for each distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "df_combined = df_combined[df_combined['outcome'].str.lower() != 'na']  \n",
    "\n",
    "desired_order = [\"uniform\", \"low\", \"high\"]  \n",
    "\n",
    "\n",
    "cards_sorted = sorted(df_combined[\"myCard\"].unique())\n",
    "dist_sorted = [d for d in desired_order if d in df_combined[\"distribution\"].unique()]\n",
    "choice_sorted = sorted(df_combined[\"choice\"].unique())\n",
    "\n",
    "\n",
    "card_idx = {card: i for i, card in enumerate(cards_sorted)}\n",
    "dist_idx = {dist: i for i, dist in enumerate(dist_sorted)}\n",
    "choice_idx = {choice: i for i, choice in enumerate(choice_sorted)}\n",
    "\n",
    "\n",
    "matrix_3d = np.zeros((len(cards_sorted), len(dist_sorted), len(choice_sorted)))\n",
    "\n",
    "\n",
    "for _, row in df_combined.iterrows():\n",
    "    i = card_idx[row[\"myCard\"]]-1\n",
    "    j = dist_idx[row[\"distribution\"]]\n",
    "    k = choice_idx[row[\"choice\"]]\n",
    "    matrix_3d[i, j, k] += 1  \n",
    "\n",
    "\n",
    "total_per_card_dist = matrix_3d.sum(axis=2, keepdims=True)\n",
    "\n",
    "# compute percentages, avoiding division by zero\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    percentage_matrix = np.divide(matrix_3d, total_per_card_dist, where=total_per_card_dist != 0)\n",
    "\n",
    "# convert to a DataFrame for easy visualization\n",
    "percentage_list = []\n",
    "for i, card in enumerate(cards_sorted):\n",
    "    for j, dist in enumerate(dist_sorted):\n",
    "        for k, choice in enumerate(choice_sorted):\n",
    "            percentage_list.append({\n",
    "                \"myCard\": card,\n",
    "                \"distribution\": dist,  # Now follows \"uniform\", \"low\", \"high\" order\n",
    "                \"choice\": choice,\n",
    "                \"percentage\": percentage_matrix[i, j, k]\n",
    "            })\n",
    "\n",
    "df_percentages = pd.DataFrame(percentage_list)\n",
    "df_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(percentage_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = { \"arrowdown\": 0, \"arrowup\": 1}\n",
    "distributions_map = { \"uniform\": 0, \"low\": 1,  \"high\": 2}\n",
    "card_numbers = list(range(1, 10))\n",
    "\n",
    "policy_table = percentage_matrix \n",
    "\n",
    "Q_table_init = np.random.normal(0, 0.1, (len(card_numbers), len(distributions_map), len(actions)))\n",
    "# having a q-table based on the policies\n",
    "Q_table_init = policy_table * np.mean(Q_table_init) \n",
    "Q_table = Q_table_init.copy()\n",
    "\n",
    "#############################################################################################\n",
    "# having a q-table that starts with 0! this was not a good initilization so i changed it.\n",
    "# Q_table = np.zeros((len(distributions_map), len(actions)))  # 3 distributions Ã— 2 actions\n",
    "#############################################################################################\n",
    "\n",
    "print(\"policy: \\n\",np.shape(policy_table))\n",
    "print(\"\\n Q_table: \\n\",np.shape(Q_table))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q_values, beta):\n",
    "    # this part subtracts the maximum q-value in each row it means each state to improve numerical stability.\n",
    "    # because exxponentials of large numbers can lead to overflow errors, so shifting q-values avoids this problem.\n",
    "    \n",
    "    Q_shifted = Q_values - np.max(Q_values, axis=2, keepdims=True)\n",
    "    exps = np.exp(beta * Q_shifted)\n",
    "    sums = np.sum(exps, axis=2, keepdims=True)\n",
    "    new_probs = exps / sums\n",
    "\n",
    "    return new_probs\n",
    "\n",
    "\n",
    "\n",
    "def train_rescorla_wagner(df, alpha, beta, Q_init=None):\n",
    "    if Q_init is None:\n",
    "        Q_init = Q_table.copy()\n",
    "    Q_values = Q_init.copy()\n",
    "    q_value_pairs = []\n",
    "    choices = []\n",
    "    predicted_probs = []\n",
    "    distributions = []\n",
    "    card_numbers = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        action = actions[row[\"choice\"]] \n",
    "        distribution = distributions_map[row[\"distribution\"]] \n",
    "        card_number = row[\"myCard\"]-1 # since I'm using this as an index! I need to do -1 to make the 1 to 9 cards come to 0 to 8\n",
    "        reward = 0.5 if row[\"outcome\"] == \"win\" else -0.5\n",
    "        \n",
    "        probs = softmax(Q_values, beta)\n",
    "        predicted_probs.append(probs[card_number][distribution][action])\n",
    "        \n",
    "        prediction_error = reward - Q_values[card_number][distribution][action]\n",
    "        Q_values[card_number][distribution][action] += alpha * prediction_error\n",
    "        \n",
    "        q_value_pairs.append(Q_values.copy())\n",
    "        choices.append(action)\n",
    "        distributions.append(distribution)\n",
    "        card_numbers.append(card_number)\n",
    "        \n",
    "\n",
    "    return np.array(q_value_pairs), np.array(choices), np.array(predicted_probs), np.array(distributions), np.array(card_numbers)\n",
    "\n",
    "\n",
    "# this is for the sake of parallel computing\n",
    "def compute_log_likelihood(alpha, beta, df_all, Q_table):\n",
    "    Q_init_participant = Q_table.copy()\n",
    "    q_values, choices, predicted_probs, distributions, card_numbers = train_rescorla_wagner(df_all, alpha, beta, Q_init=Q_init_participant)\n",
    "    \n",
    "    predicted_probs = np.clip(predicted_probs, 1e-6, 1)  # prevent log(0)\n",
    "    log_likelihood = np.sum(np.log(predicted_probs))\n",
    "    \n",
    "    return (alpha, beta, log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93538c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = 500\n",
    "# num_of_samples = 1000\n",
    "alpha_min = 0\n",
    "alpha_max = 1\n",
    "beta_min = 0\n",
    "beta_max  = 8\n",
    "alpha_samples = np.random.uniform(alpha_min, alpha_max + np.finfo(float).eps, num_of_samples)\n",
    "beta_samples = np.random.uniform(beta_min, beta_max + np.finfo(float).eps, num_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIC_models = []\n",
    "\n",
    "for idx, df_all in enumerate(dataframes):\n",
    "    Q_init_participant = Q_table.copy()\n",
    "    \n",
    "    df_all = df_all[df_all['outcome'].str.lower() != 'na']  \n",
    "\n",
    "\n",
    "    best_alpha, best_beta = None, None\n",
    "    best_log_likelihood = -np.inf\n",
    "\n",
    "\n",
    "    alpha_beta_log_likelihood = {}\n",
    "\n",
    "    results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(compute_log_likelihood)(alpha, beta, df_all, Q_table) \n",
    "    for alpha in alpha_samples for beta in beta_samples)\n",
    "\n",
    "\n",
    "# finding alpha beta in parallel way\n",
    "    alpha_beta_log_likelihood = {}\n",
    "    best_log_likelihood = -np.inf\n",
    "    best_alpha, best_beta = None, None\n",
    "\n",
    "    for alpha, beta, log_likelihood in results:\n",
    "        alpha_beta_log_likelihood[(alpha, beta)] = log_likelihood\n",
    "        if log_likelihood > best_log_likelihood:\n",
    "            best_log_likelihood = log_likelihood\n",
    "            best_alpha, best_beta = alpha, beta\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(alpha_beta_log_likelihood.keys(), columns=[\"alpha\", \"beta\"])\n",
    "    results_df[\"log_likelihood\"] = alpha_beta_log_likelihood.values()\n",
    "\n",
    "    #  model prediction \n",
    "    \n",
    "    q_values, choices, predicted_probs, distributions, card_numbers = train_rescorla_wagner(df_all, best_alpha, best_beta, Q_init=Q_init_participant)\n",
    "    # now we need to find out the predicted choices of the model:\n",
    "    \n",
    "\n",
    "    predicted_choices = []\n",
    "\n",
    "    for trial in range(len(distributions)):  \n",
    "        if q_values[trial][card_numbers[trial]][distributions[trial]][actions[\"arrowup\"]] > q_values[trial][card_numbers[trial]][distributions[trial]][actions[\"arrowdown\"]]:\n",
    "            predicted_choices.append(1)\n",
    "        else:\n",
    "            predicted_choices.append(0)\n",
    "\n",
    "    \n",
    "    \n",
    "    # confusion matrix\n",
    "    conf_matrix = confusion_matrix(choices, predicted_choices)\n",
    "    \n",
    "    # bayes information criterion\n",
    "    n_trials = len(df_all)\n",
    "    k = 2  # number of free parameters: alpha and beta\n",
    "    BIC = k * np.log(n_trials) - 2 * best_log_likelihood # this is BIC formula based on the log lkelihode I found before\n",
    "\n",
    "    # print(BIC)\n",
    "\n",
    "    BIC_models.append(BIC)\n",
    "\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "    ## visulization\n",
    "    ###########################################################################################\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(19, 6))\n",
    "\n",
    "    plots_smooth_level = 20\n",
    "\n",
    "\n",
    "#############################################\n",
    "    # Density Plot (KDE)\n",
    "    sns.kdeplot(\n",
    "        x=results_df[\"alpha\"], \n",
    "        y=results_df[\"beta\"], \n",
    "        fill=True, \n",
    "        cmap=\"viridis\", \n",
    "        ax=axes[0], \n",
    "        bw_adjust=1.8,  # Increase for smoother density\n",
    "        levels=plots_smooth_level,  # More contour levels\n",
    "        thresh=0  # Ensure density is plotted across all values\n",
    "    )\n",
    "    mappable = axes[0].collections[0]\n",
    "    cbar = fig.colorbar(mappable, ax=axes[0], label=\"density\", fraction=0.046, pad=0.04)  \n",
    "    cbar.ax.yaxis.set_major_formatter(mticker.FormatStrFormatter('%.2f'))  # 2 decimal places\n",
    "    cbar.ax.set_ylabel(\"density\", fontsize=12, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "    axes[0].set_xlim(alpha_min, alpha_max)\n",
    "    axes[0].set_ylim(beta_min, beta_max)\n",
    "    axes[0].set_xlabel(\"learning rate (Î±)\", fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel(\"inverse temp (Î²)\", fontsize=14, fontweight='bold')\n",
    "    axes[0].set_title(\"density of Î± and Î² joint probability\", fontsize=16, fontweight='bold')\n",
    "    axes[0].tick_params(axis='both', labelsize=14)\n",
    "#############################################\n",
    "\n",
    "\n",
    "#############################################\n",
    "    # Log Likelihood \n",
    "\n",
    "    alpha_step = 0.1\n",
    "    beta_step = 1\n",
    "    alpha_bins = np.arange(alpha_min, alpha_max+ alpha_step, alpha_step)  # bins from 0 to 1 with step 0.1\n",
    "    beta_bins = np.arange(beta_min, beta_max + beta_step, beta_step)       # bins from 0 to 8 with step 1\n",
    "\n",
    "    results_df[\"alpha_binned\"] = pd.cut(results_df[\"alpha\"], bins=alpha_bins, labels=alpha_bins[:-1], include_lowest=True)\n",
    "    results_df[\"beta_binned\"] = pd.cut(results_df[\"beta\"], bins=beta_bins, labels=beta_bins[:-1], include_lowest=True)\n",
    "\n",
    "    heatmap_data = results_df.groupby(\n",
    "    [\"beta_binned\", \"alpha_binned\"], observed=False)[\"log_likelihood\"].mean().unstack()\n",
    "\n",
    "    heatmap_data.index = heatmap_data.index.astype(int)\n",
    "    heatmap_data.columns = heatmap_data.columns.astype(float)\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        cmap=\"Blues\", \n",
    "        cbar=True,\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_xticks(np.arange(len(heatmap_data.columns)))  \n",
    "    axes[1].set_xticklabels([f\"{x:.1f}\" for x in heatmap_data.columns], rotation=45)\n",
    "\n",
    "    axes[1].set_yticks(np.arange(len(heatmap_data.index))) \n",
    "    axes[1].set_yticklabels(heatmap_data.index) \n",
    "\n",
    "\n",
    "    axes[1].set_xlabel(\"learning rate (Î±)\", fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel(\"inverse temp (Î²)\", fontsize=14, fontweight='bold')\n",
    "    axes[1].set_title(\"log likelihood for combinations of Î± and Î²\", fontsize=16, fontweight='bold')\n",
    "    axes[1].tick_params(axis='both', labelsize=14)\n",
    "    axes[1].invert_yaxis()\n",
    "\n",
    "    \n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "#############################################\n",
    "    # Confusion Matrix\n",
    "    heatmap_cmap_color = mcolors.LinearSegmentedColormap.from_list(\"warm_red\", [\"#fff5e6\", \"#ff5733\"])\n",
    "    sns.heatmap(\n",
    "        conf_matrix, annot=True, fmt=\"d\", cmap=heatmap_cmap_color,\n",
    "        xticklabels=[\"arrowdown\", \"arrowup\"], \n",
    "        yticklabels=[\"arrowdown\", \"arrowup\"], \n",
    "        ax=axes[2], \n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    axes[2].set_xlabel(\"prediction\", fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel(\"true label\", fontsize=14, fontweight='bold')\n",
    "    axes[2].set_title(f\"confusion matrix (Î±={best_alpha:.2f}, Î²={best_beta:.2f})\", fontsize=16, fontweight='bold')\n",
    "    axes[2].tick_params(axis='both', labelsize=14)\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9]) \n",
    "    fig.suptitle(f'participant {idx}', fontsize=18, fontweight='bold', y=0.95)\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"plot_{idx}.pdf\")\n",
    "    plt.savefig(filename, format='pdf')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"saved: {filename}\")\n",
    "\n",
    "\n",
    "# # saving BIC_models to compare models:\n",
    "# file_path_BIC = os.path.join(output_dir, \"BIC_models_see_numbers_and_colors.txt\")\n",
    "\n",
    "# with open(file_path_BIC, \"w\") as file:\n",
    "#     for bic in BIC_models:\n",
    "#         file.write(f\"{bic}\\n\")\n",
    "\n",
    "# print(f\"BIC values saved to {file_path_BIC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0bedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bb9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
