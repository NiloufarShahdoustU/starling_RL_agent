{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a6988d",
   "metadata": {},
   "source": [
    "# TD learning; the agent that can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7001df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(42)\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228a8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 31 participants.\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data_risk_added'\n",
    "dataframes = [pd.read_excel(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.xlsx')]\n",
    "\n",
    "n_participant = len(dataframes)\n",
    "print(f\"there are {n_participant} participants.\")\n",
    "\n",
    "\n",
    "output_dir = \"11_RL_agent_TDlearn_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4cf94",
   "metadata": {},
   "source": [
    "# policy initilization for the model\n",
    "now I need to find the prior policy amounts. for that I am going to put the percentage of downarrow and up arrow for each distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4d584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "distribution",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arrowdown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "arrowup",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "89772307-e5fb-4833-a8e9-26b34e177450",
       "rows": [
        [
         "high",
         "0.46200716845878137",
         "0.5379928315412187"
        ],
        [
         "low",
         "0.5555555555555556",
         "0.4444444444444444"
        ],
        [
         "uniform",
         "0.4989247311827957",
         "0.5010752688172043"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>choice</th>\n",
       "      <th>arrowdown</th>\n",
       "      <th>arrowup</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distribution</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.462007</td>\n",
       "      <td>0.537993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniform</th>\n",
       "      <td>0.498925</td>\n",
       "      <td>0.501075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "choice        arrowdown   arrowup\n",
       "distribution                     \n",
       "high           0.462007  0.537993\n",
       "low            0.555556  0.444444\n",
       "uniform        0.498925  0.501075"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.concat(dataframes, ignore_index=True)\n",
    "df_combined = df_combined[df_combined['outcome'].str.lower() != 'na']  \n",
    "\n",
    "count_df = df_combined.pivot_table(index=\"distribution\", columns=\"choice\", aggfunc=\"size\", fill_value=0)\n",
    "policy_initialization_df = count_df.div(count_df.sum(axis=1), axis=0)\n",
    "policy_initialization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065cd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = { \"arrowdown\": 0, \"arrowup\": 1}\n",
    "distributions_map = { \"uniform\": 0, \"low\": 1,  \"high\": 2}\n",
    "\n",
    "policy_table = policy_initialization_df.rename(index=distributions_map, columns=actions).sort_index().to_numpy()\n",
    "\n",
    "Q_table_init = np.random.normal(0, 0.1, (len(distributions_map), len(actions)))\n",
    "\n",
    "\n",
    "# having a q-table based on the policies\n",
    "Q_table_init = policy_table * np.mean(Q_table_init) \n",
    "Q_table = Q_table_init.copy()\n",
    "\n",
    "# having a q-table that starts with 0!\n",
    "# Q_table = np.zeros((len(distributions_map), len(actions)))  # 3 distributions × 2 actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b4d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q_values, beta):    \n",
    "    # this part subtracts the maximum q-value in each row it means each state to improve numerical stability.\n",
    "    # because exxponentials of large numbers can lead to overflow errors, so shifting q-values avoids this problem.\n",
    "    \n",
    "    Q_shifted = Q_values - np.max(Q_values, axis=1, keepdims=True)\n",
    "    exps = np.exp(beta * Q_shifted)\n",
    "    sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    new_probs = exps / sums\n",
    "\n",
    "    return new_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_rescorla_wagner(df, alpha, beta , Q_init=None):\n",
    "    if Q_init is None:\n",
    "        Q_init = Q_table.copy()\n",
    "    Q_values = Q_init.copy()\n",
    "    \n",
    "    q_value_pairs = []\n",
    "    choices = []\n",
    "    predicted_probs = []\n",
    "    distributions = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        action = actions[row[\"choice\"]] \n",
    "        distribution = distributions_map[row[\"distribution\"]] \n",
    "        reward = 0.5 if row[\"outcome\"] == \"win\" else -0.5\n",
    "        \n",
    "        probs = softmax(Q_values, beta)\n",
    "        predicted_probs.append(probs[distribution][action])\n",
    "        \n",
    "        prediction_error = reward - Q_values[distribution][action]\n",
    "        Q_values[distribution][action] += alpha * prediction_error\n",
    "        \n",
    "        q_value_pairs.append(Q_values.copy())\n",
    "        choices.append(action)\n",
    "        distributions.append(distribution)\n",
    "        \n",
    "\n",
    "    return np.array(q_value_pairs), np.array(choices), np.array(predicted_probs), np.array(distributions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0f6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: 11_RL_agent_TDlearn_output/plot_0.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_1.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_2.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_3.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_4.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_5.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_6.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_7.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_8.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_9.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_10.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_11.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_12.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_13.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_14.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_15.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_16.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_17.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_18.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_19.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_20.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_21.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_22.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_23.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_24.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_25.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_26.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_27.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_28.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_29.pdf\n",
      "saved: 11_RL_agent_TDlearn_output/plot_30.pdf\n",
      "BIC values saved to 11_RL_agent_TDlearn_output/BIC_models_sees.txt\n"
     ]
    }
   ],
   "source": [
    "BIC_models = []\n",
    "for idx, df_all in enumerate(dataframes):\n",
    "    \n",
    "    df_all = df_all[df_all['outcome'].str.lower() != 'na']  \n",
    "\n",
    "    num_of_samples = 50\n",
    "    alpha_samples = np.random.uniform(0.01, 1, num_of_samples) \n",
    "    beta_samples = np.random.uniform(0, 8, num_of_samples)  \n",
    "    \n",
    "    Q_init_participant = Q_table.copy()\n",
    "\n",
    "    best_alpha, best_beta = None, None\n",
    "    best_log_likelihood = -np.inf\n",
    "\n",
    "\n",
    "    alpha_beta_log_likelihood = {}\n",
    "\n",
    "\n",
    "    # finding alpha beta\n",
    "\n",
    "    for alpha in alpha_samples:\n",
    "        for beta in beta_samples:\n",
    "            Q_init_participant = Q_table.copy()\n",
    "            q_values, choices, predicted_probs, distributions = train_rescorla_wagner(df_all, alpha, beta, Q_init=Q_init_participant)\n",
    "            \n",
    "            predicted_probs = np.clip(predicted_probs, 1e-6, 1)  # prevent log(0)\n",
    "            log_likelihood = np.sum(np.log(predicted_probs))\n",
    "            alpha_beta_log_likelihood[(alpha, beta)] = log_likelihood\n",
    "            \n",
    "            \n",
    "            if log_likelihood > best_log_likelihood: # find best alpha beta\n",
    "                best_log_likelihood = log_likelihood\n",
    "                best_alpha, best_beta = alpha, beta\n",
    "\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(alpha_beta_log_likelihood.keys(), columns=[\"alpha\", \"beta\"])\n",
    "    results_df[\"log_likelihood\"] = alpha_beta_log_likelihood.values()\n",
    "\n",
    "    #  model prediction \n",
    "    \n",
    "    q_values, choices, predicted_probs, distributions = train_rescorla_wagner(df_all, best_alpha, best_beta, Q_init=Q_init_participant)\n",
    "    # now we need to find out the predicted choices of the model:\n",
    "    \n",
    "\n",
    "    predicted_choices = []\n",
    "\n",
    "    for trial in range(len(distributions)):  \n",
    "        if q_values[trial][distributions[trial]][actions[\"arrowup\"]] > q_values[trial][distributions[trial]][actions[\"arrowdown\"]]:\n",
    "            predicted_choices.append(1)\n",
    "        else:\n",
    "            predicted_choices.append(0)\n",
    "\n",
    "    \n",
    "    \n",
    "    # confusion matrix\n",
    "    conf_matrix = confusion_matrix(choices, predicted_choices)\n",
    "    \n",
    "    # bayes information criterion\n",
    "    n_trials = len(df_all)\n",
    "    k = 2  # number of free parameters: alpha and beta\n",
    "    BIC = k * np.log(n_trials) - 2 * best_log_likelihood\n",
    "    \n",
    "    BIC_models.append(BIC)\n",
    "    ###########################################################################################\n",
    "    ## visulization\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "\n",
    "    sns.kdeplot(x=results_df[\"alpha\"], y=results_df[\"beta\"], fill=True, cmap=\"viridis\", ax=axes[0])\n",
    "    mappable = axes[0].collections[0]\n",
    "    fig.colorbar(mappable, ax=axes[0], label=\"density\")\n",
    "    axes[0].set_xlabel(\"learning rate (alpha)\")\n",
    "    axes[0].set_ylabel(\"inverse temp (beta)\")\n",
    "    axes[0].set_title(\"density of alpha beta joint probability\")\n",
    "\n",
    "\n",
    "    scatter = sns.scatterplot(x=results_df[\"alpha\"], y=results_df[\"beta\"], \n",
    "                            size=results_df[\"log_likelihood\"], hue=results_df[\"log_likelihood\"], \n",
    "                            palette=\"Blues\", ax=axes[1], legend=False) \n",
    "\n",
    "    norm = plt.Normalize(results_df[\"log_likelihood\"].min(), results_df[\"log_likelihood\"].max())\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"Blues\", norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=axes[1], label=\"log likelihood\")\n",
    "\n",
    "    axes[1].set_ylabel(\"inverse temp (beta)\")\n",
    "    axes[1].set_xlabel(\"learning rate (alpha)\")\n",
    "    axes[1].set_title(\"log likelihood scatterplot\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"arrowdown\", \"arrowup\"], \n",
    "                yticklabels=[\"arrowdown\", \"arrowup\"], ax=axes[2], cbar=False)\n",
    "    axes[2].set_xlabel(\"prediction\")\n",
    "    axes[2].set_ylabel(\"true label\")\n",
    "    axes[2].set_title(f\"confusion matrix (α={best_alpha:.2f}, β={best_beta:.2f})\")\n",
    "    axes[2].text(0.5, -0.3, f\"BIC: {BIC:.2f}\", fontsize=12, fontweight=\"bold\",\n",
    "            ha=\"center\", va=\"center\", transform=axes[2].transAxes)\n",
    "    \n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9]) \n",
    "    fig.suptitle(f'participant {idx}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"plot_{idx}.pdf\")\n",
    "    plt.savefig(filename, format='pdf')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"saved: {filename}\")\n",
    "\n",
    "\n",
    "# saving BIC_models to compare models:\n",
    "file_path_BIC = os.path.join(output_dir, \"BIC_models_sees.txt\")\n",
    "\n",
    "with open(file_path_BIC, \"w\") as file:\n",
    "    for bic in BIC_models:\n",
    "        file.write(f\"{bic}\\n\")\n",
    "\n",
    "print(f\"BIC values saved to {file_path_BIC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0bedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
